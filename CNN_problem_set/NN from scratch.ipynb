{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NN for Binary Classification ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beaee94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d1dc9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861ab0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2e1cef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X, y = datasets.make_moons(100, noise=0.10)\n",
    "x1 = X[:,0]\n",
    "x2 = X[:,1]\n",
    "y = y.reshape(y.shape[0],1)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Initialize weights and biases in NN ###\n",
    "\n",
    "def define_parameters(weights):\n",
    "    weight_list = []\n",
    "    bias_list = []\n",
    "    \n",
    "    for i in range(len(weights) - 1):\n",
    "        w = np.random.randn(weights[i], weights[i+1])\n",
    "        b = np.random.randn()\n",
    "        weight_list.append(w)\n",
    "        bias_list.append(b)\n",
    "        \n",
    "    return weight_list, bias_list\n",
    "\n",
    "### activation function ###\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "### feed forward algorithm ###\n",
    "\n",
    "def predictions(w, b, X):\n",
    "    zh = np.dot(X,w[0]) + b[0]\n",
    "    ah = sigmoid(zh)\n",
    "    zo = np.dot(ah, w[1]) + b[1]\n",
    "    ao = sigmoid(zo)\n",
    "    return ao\n",
    "\n",
    "### cost function (MSE) ###\n",
    "\n",
    "def find_cost(ao,y):\n",
    "    m = y.shape[0]\n",
    "    total_cost = (1/m) * np.sum(np.square(ao - y))\n",
    "    return total_cost\n",
    "\n",
    "### back propagation ###\n",
    "\n",
    "def find_derivatives(w, b, X):\n",
    "    zh = np.dot(X,w[0]) + b[0]\n",
    "    ah = sigmoid(zh)\n",
    "    \n",
    "    zo = np.dot(ah, w[1]) + b[1]\n",
    "    ao = sigmoid(zo)\n",
    "\n",
    "    # Backpropagation phase 1\n",
    "    m = y.shape[0]\n",
    "    dcost_dao = (1/m)*(ao-y)\n",
    "    dao_dzo = sigmoid_der(zo)\n",
    "    dzo_dwo = ah.T\n",
    "\n",
    "    dwo = np.dot(dzo_dwo, dcost_dao * dao_dzo)\n",
    "    dbo = np.sum(dcost_dao * dao_dzo)\n",
    "    \n",
    "    # Backpropagation phase 2\n",
    "\n",
    "    # dcost_wh = dcost_dah * dah_dzh * dzh_dwh\n",
    "    # dcost_dah = dcost_dzo * dzo_dah\n",
    "\n",
    "    dcost_dzo = dcost_dao * dao_dzo\n",
    "    dzo_dah = w[1].T\n",
    "\n",
    "    dcost_dah = np.dot(dcost_dzo , dzo_dah)\n",
    "\n",
    "    dah_dzh = sigmoid_der(zh)\n",
    "    dzh_dwh = X.T\n",
    "    dwh = np.dot(dzh_dwh, dah_dzh * dcost_dah)\n",
    "    dbh = np.sum(dah_dzh * dcost_dah)\n",
    "\n",
    "    return dwh, dbh, dwo, dbo\n",
    "\n",
    "### Update weights from gradients ###\n",
    "\n",
    "def update_weights(w,b,dwh, dbh, dwo, dbo, lr):\n",
    "    w[0] = w[0] - lr * dwh\n",
    "    w[1] = w[1] - lr * dwo\n",
    "\n",
    "    b[0] = b[0] - lr * dbh\n",
    "    b[1] = b[1] - lr * dbo\n",
    "\n",
    "    return w, b\n",
    "\n",
    "### NN class for training ###\n",
    "\n",
    "def my_neural_network(X, y, lr, epochs):\n",
    "    error_list = []\n",
    "    input_len = X.shape[1]\n",
    "    output_len = y.shape[1]\n",
    "    w,b = define_parameters([input_len, 4, output_len])\n",
    "\n",
    "    for i in range(epochs):\n",
    "        ao = predictions(w, b, X)\n",
    "        cost = find_cost(ao, y)\n",
    "        error_list.append(cost)\n",
    "        dwh, dbh, dwo, dbo = find_derivatives (w, b, X)\n",
    "        w, b = update_weights(w, b, dwh, dbh, dwo, dbo, lr )\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(cost)\n",
    "    return w, b, error_list\n",
    "\n",
    "### set hyperparameters and train NN ###\n",
    "\n",
    "lr = 0.5\n",
    "epochs = 10\n",
    "w, b, error_list = my_neural_network(X,y,lr,epochs)\n",
    "print('weight:', w)\n",
    "print('bias:', b)\n",
    "print('last 10 cost values:', error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21fc1c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NN for multiclass Classification 3 outputs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434168c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "cat1 = np.random.randn(800, 2) + np.array([0, -3])\n",
    "cat2 = np.random.randn(800, 2) + np.array([3, 3])\n",
    "cat3 = np.random.randn(800, 2) + np.array([-3, 3])\n",
    "\n",
    "X = np.vstack([cat1, cat2, cat3])\n",
    "\n",
    "labels = np.array([0]*800 + [1]*800 + [2]*800)\n",
    "\n",
    "y = np.zeros((2400, 3))\n",
    "\n",
    "for i in range(2400):\n",
    "    y[i, labels[i]] = 1\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "x1 = X[:,0]\n",
    "x2 = X[:,1]\n",
    "\n",
    "### Initialize weights and biases in NN ###\n",
    "\n",
    "def define_parameters(weights):\n",
    "    weight_list = []\n",
    "    bias_list = []\n",
    "    for i in range(len(weights) - 1):\n",
    "        w = np.random.randn(weights[i], weights[i+1])\n",
    "        b = np.random.randn(weights[i+1])\n",
    "        weight_list.append(w)\n",
    "        bias_list.append(b)\n",
    "    return weight_list, bias_list\n",
    "\n",
    "### output layer(softmax function) ###\n",
    "\n",
    "def softmax(X):\n",
    "    expX = np.exp(X)\n",
    "    return expX / expX.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "### activation function ###\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "### feed forward ###\n",
    "\n",
    "def predictions(w, b, X):\n",
    "    zh = np.dot(X,w[0]) + b[0]\n",
    "    ah = sigmoid(zh)\n",
    "\n",
    "    zo = np.dot(ah, w[1]) + b[1]\n",
    "    ao = softmax(zo)\n",
    "    return ao\n",
    "\n",
    "\n",
    "### Cost Functions ###\n",
    "\n",
    "def find_cost(ao,y):\n",
    "\n",
    "    total_cost = np.sum(-y * np.log(ao))\n",
    "    return total_cost\n",
    "\n",
    "### backpropagation ###\n",
    "\n",
    "def find_derivatives(w, b, X):\n",
    "\n",
    "    zh = np.dot(X,w[0]) + b[0]\n",
    "\n",
    "    ah = sigmoid(zh)\n",
    "\n",
    "    zo = np.dot(ah, w[1]) + b[1]\n",
    "    ao = softmax(zo)\n",
    "\n",
    "    # Back propagation phase 1\n",
    "\n",
    "\n",
    "    dcost_dzo = (ao-y)\n",
    "    dzo_dwo = ah.T\n",
    "\n",
    "    dwo = np.dot(dzo_dwo, dcost_dzo)\n",
    "    dbo = np.sum(dcost_dzo)\n",
    "\n",
    "    # Back propagation phase 2\n",
    "\n",
    "    # dcost_wh = dcost_dah * dah_dzh * dzh_dwh\n",
    "    # dcost_dah = dcost_dzo * dzo_dah\n",
    "\n",
    "\n",
    "    dzo_dah = w[1].T\n",
    "\n",
    "    dcost_dah = np.dot(dcost_dzo , dzo_dah)\n",
    "\n",
    "    dah_dzh = sigmoid_der(zh)\n",
    "    dzh_dwh = X.T\n",
    "    dwh = np.dot(dzh_dwh, dah_dzh * dcost_dah)\n",
    "    dbh = np.sum(dah_dzh * dcost_dah)\n",
    "\n",
    "    return dwh, dbh, dwo, dbo\n",
    "\n",
    "def update_weights(w,b,dwh, dbh, dwo, dbo, lr):\n",
    "    w[0] = w[0] - lr * dwh\n",
    "    w[1] = w[1] - lr * dwo\n",
    "\n",
    "    b[0] = b[0] - lr * dbh\n",
    "    b[1] = b[1] - lr * dbo\n",
    "\n",
    "    return w, b\n",
    "\n",
    "### Train NN ###\n",
    "\n",
    "def my_multiout_neural_network(X, y, lr, epochs):\n",
    "\n",
    "    error_list = []\n",
    "    input_len = X.shape[1]  # 784 for MNIST\n",
    "    output_len = y.shape[1] # 10 for MNIST\n",
    "    w,b = define_parameters([input_len, 4, output_len])\n",
    "\n",
    "\n",
    "    for i in range(epochs):\n",
    "        ao = predictions(w, b, X)\n",
    "        cost = find_cost(ao, y)\n",
    "        error_list.append(cost)\n",
    "        dwh, dbh, dwo, dbo = find_derivatives (w, b, X)\n",
    "        w, b = update_weights(w, b, dwh, dbh, dwo, dbo,lr)\n",
    "        if i % 50 == 0 :\n",
    "            print(cost)\n",
    "\n",
    "    return w, b, error_list\n",
    "\n",
    "\n",
    "lr = 0.0005\n",
    "epochs = 10\n",
    "w, b, error_list = my_multiout_neural_network(X,y,lr,epochs)\n",
    "print(error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913774e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NN for multiclass Classification for mnist dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2893f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to load MNIST data from npz file\n",
    "def load_mnist_from_npz(filename):\n",
    "    with np.load(filename, allow_pickle=True) as npzfile:\n",
    "        X_train = npzfile['x_train']\n",
    "        y_train = npzfile['y_train']\n",
    "        X_test = npzfile['x_test']\n",
    "        y_test = npzfile['y_test']\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "# The mnist.npz file should be in the same directory as this script\n",
    "mnist_npz_filename = 'mnist.npz'\n",
    "\n",
    "# Load the data\n",
    "(X_train, y_train), (X_test, y_test) = load_mnist_from_npz(mnist_npz_filename)\n",
    "\n",
    "# Reshaping and normalizing the data\n",
    "X_train = X_train.reshape(-1, 28, 28) / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28) / 255.0\n",
    "\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_one_hot = np.zeros((len(y_train), 10))\n",
    "y_train_one_hot[np.arange(len(y_train)), y_train] = 1\n",
    "\n",
    "\n",
    "y_test_one_hot = np.zeros((len(y_test), 10))\n",
    "y_test_one_hot[np.arange(len(y_test)), y_test] = 1\n",
    "\n",
    "# Flatten the images for input to the neural network\n",
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# ... MNIST data loading and preprocessing code ...\n",
    "\n",
    "### Initialize weights and biases in NN ###\n",
    "\n",
    "def define_parameters(weights):\n",
    "    weight_list = []\n",
    "    bias_list = []\n",
    "    for i in range(len(weights) - 1):\n",
    "        w = np.random.randn(weights[i], weights[i+1]) * np.sqrt(1. / weights[i])\n",
    "        b = np.zeros((weights[i+1]))\n",
    "        weight_list.append(w)\n",
    "        bias_list.append(b)\n",
    "    return weight_list, bias_list\n",
    "\n",
    "### output layer(softmax function) ###\n",
    "\n",
    "def softmax(X):\n",
    "    expX = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "    return expX / expX.sum(axis=1, keepdims=True)\n",
    "\n",
    "### activation function ###\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "### feed forward ###\n",
    "\n",
    "def predictions(w, b, X):\n",
    "    zh1 = np.dot(X, w[0]) + b[0]\n",
    "    ah1 = sigmoid(zh1)\n",
    "\n",
    "    zh2 = np.dot(ah1, w[1]) + b[1]\n",
    "    ah2 = sigmoid(zh2)\n",
    "\n",
    "    zo = np.dot(ah2, w[2]) + b[2]\n",
    "    ao = softmax(zo)\n",
    "    # Return both activations and pre-activation values\n",
    "    return zh1, ah1, zh2, ah2, ao\n",
    "\n",
    "\n",
    "### Cost Functions ###\n",
    "\n",
    "def find_cost(ao, y):\n",
    "    m = y.shape[0]\n",
    "    total_cost = -np.sum(y * np.log(ao)) / m\n",
    "    return total_cost\n",
    "\n",
    "### backpropagation ###\n",
    "\n",
    "def find_derivatives(w, b, X, y, zh1, ah1, zh2, ah2, ao):\n",
    "    m = y.shape[0]\n",
    "\n",
    "    # Back propagation phase 1\n",
    "    dcost_dzo = ao - y\n",
    "    dzo_dw2 = ah2.T\n",
    "    dw2 = np.dot(dzo_dw2, dcost_dzo) / m\n",
    "    db2 = np.sum(dcost_dzo, axis=0) / m\n",
    "\n",
    "    # Phase 2\n",
    "    dcost_dah2 = np.dot(dcost_dzo, w[2].T)\n",
    "    dah2_dzh2 = sigmoid_der(zh2)\n",
    "    dzh2_dw1 = ah1.T\n",
    "    dw1 = np.dot(dzh2_dw1, dcost_dah2 * dah2_dzh2) / m\n",
    "    db1 = np.sum(dcost_dah2 * dah2_dzh2, axis=0) / m\n",
    "\n",
    "    # Phase 3\n",
    "    dcost_dah1 = np.dot(dcost_dah2 * dah2_dzh2, w[1].T)\n",
    "    dah1_dzh1 = sigmoid_der(zh1)\n",
    "    dzh1_dw0 = X.T\n",
    "    dw0 = np.dot(dzh1_dw0, dcost_dah1 * dah1_dzh1) / m\n",
    "    db0 = np.sum(dcost_dah1 * dah1_dzh1, axis=0) / m\n",
    "\n",
    "    return dw0, db0, dw1, db1, dw2, db2\n",
    "\n",
    "def update_weights(w, b, dw0, db0, dw1, db1, dw2, db2, lr):\n",
    "    w[0] = w[0] - lr * dw0\n",
    "    b[0] = b[0] - lr * db0\n",
    "\n",
    "    w[1] = w[1] - lr * dw1\n",
    "    b[1] = b[1] - lr * db1\n",
    "\n",
    "    w[2] = w[2] - lr * dw2\n",
    "    b[2] = b[2] - lr * db2\n",
    "\n",
    "    return w, b\n",
    "\n",
    "### Train NN ###\n",
    "\n",
    "def my_multiout_neural_network(X, y, lr, epochs):\n",
    "    error_list = []\n",
    "    input_len = X.shape[1]  # 784 for MNIST\n",
    "    output_len = y.shape[1] # 10 for MNIST\n",
    "    w, b = define_parameters([input_len, 200, 50, output_len])\n",
    "\n",
    "    for i in range(epochs):\n",
    "        zh1, ah1, zh2, ah2, ao = predictions(w, b, X)\n",
    "        cost = find_cost(ao, y)\n",
    "        error_list.append(cost)\n",
    "        # Pass zh1 and zh2 to find_derivatives\n",
    "        dw0, db0, dw1, db1, dw2, db2 = find_derivatives(w, b, X, y, zh1, ah1, zh2, ah2, ao)\n",
    "        w, b = update_weights(w, b, dw0, db0, dw1, db1, dw2, db2, lr)\n",
    "        if i % 50 == 0 :\n",
    "            print(f'Epoch {i}: cost = {cost}')\n",
    "\n",
    "    return w, b, error_list\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "lr = 0.005\n",
    "epochs = 10\n",
    "w, b, error_list = my_multiout_neural_network(X_train_flattened, y_train_one_hot, lr, epochs)\n",
    "print(error_list)\n",
    "\n",
    "# Evaluate on test data\n",
    "# You would typically have an evaluation function here\n",
    "# For simplicity, we will just print the shape of the test data\n",
    "print(X_test_flattened.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "### updated based on pytorch implementation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac73c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: cost = 0.5483631522608613\n",
      "Training Time: 43.403562784194946 seconds\n",
      "Train Accuracy: 94.07666666666667%\n",
      "Test Accuracy: 93.95%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def load_and_preprocess_mnist(filename):\n",
    "    # Load data\n",
    "    with np.load(filename, allow_pickle=True) as npzfile:\n",
    "        X_train = npzfile['x_train']\n",
    "        y_train = npzfile['y_train']\n",
    "        X_test = npzfile['x_test']\n",
    "        y_test = npzfile['y_test']\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "    \n",
    "    X_train_normalized = X_train.reshape(-1, 784) / 255.0\n",
    "    X_test_normalized = X_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "    # Shift to have a mean of 0 and standard deviation of 1\n",
    "    X_train_normalized = (X_train_normalized - np.mean(X_train_normalized)) / np.std(X_train_normalized)\n",
    "    X_test_normalized = (X_test_normalized - np.mean(X_test_normalized)) / np.std(X_test_normalized)\n",
    "    \n",
    "    # One-hot encode labels in vector form\n",
    "    y_train_one_hot = np.eye(10)[y_train]\n",
    "    y_test_one_hot = np.eye(10)[y_test]\n",
    "\n",
    "    return (X_train, y_train_one_hot), (X_test, y_test_one_hot)\n",
    "\n",
    "mnist_npz_filename = 'mnist.npz'\n",
    "(X_train, y_train), (X_test, y_test) = load_and_preprocess_mnist(mnist_npz_filename)\n",
    "\n",
    "        \n",
    "### summary statistics ###\n",
    "def calculate_accuracy(X, y, w, b):\n",
    "    _, _, _, _, pred = predictions(w, b, X) \n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    labels = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(pred == labels)\n",
    "    return accuracy * 100\n",
    "\n",
    "\n",
    "### Initialize weights and biases in NN ###\n",
    "\n",
    "def define_parameters(weights):\n",
    "    weight_list = []\n",
    "    bias_list = []\n",
    "    for i in range(len(weights) - 1):\n",
    "        w = np.random.randn(weights[i], weights[i+1]) * np.sqrt(1. / weights[i])\n",
    "        b = np.zeros((weights[i+1]))\n",
    "        weight_list.append(w)\n",
    "        bias_list.append(b)\n",
    "    return weight_list, bias_list\n",
    "\n",
    "### output layer(softmax function) ###\n",
    "\n",
    "def softmax(X):\n",
    "    expX = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "    return expX / expX.sum(axis=1, keepdims=True)\n",
    "\n",
    "### activation function ReLU ###\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_der(x):\n",
    "    return (x > 0).astype(x.dtype)\n",
    "\n",
    "### feed forward with ReLU ###\n",
    "def predictions(w, b, X):\n",
    "    zh1 = np.dot(X, w[0]) + b[0]\n",
    "    ah1 = relu(zh1)\n",
    "\n",
    "    zh2 = np.dot(ah1, w[1]) + b[1]\n",
    "    ah2 = relu(zh2)\n",
    "\n",
    "    zo = np.dot(ah2, w[2]) + b[2]\n",
    "    ao = softmax(zo)\n",
    "    return zh1, ah1, zh2, ah2, ao\n",
    "\n",
    "\n",
    "### stochastic gradient descent ### \n",
    "def batch_generator(X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        yield X[i:i + batch_size], y[i:i + batch_size]\n",
    "\n",
    "### Cross-Entropy Loss ###\n",
    "def cross_entropy_loss(ao, y):\n",
    "    m = y.shape[0]\n",
    "    # Adding a small number to avoid log(0)\n",
    "    return -np.sum(y * np.log(ao + 1e-9)) / m\n",
    "\n",
    "### Backpropagation with ReLU ###\n",
    "def find_derivatives(w, b, X, y, zh1, ah1, zh2, ah2, ao):\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # phase 1\n",
    "    dcost_dzo = ao - y\n",
    "    dzo_dw2 = ah2.T\n",
    "    dw2 = np.dot(dzo_dw2, dcost_dzo) / m\n",
    "    db2 = np.sum(dcost_dzo, axis=0) / m\n",
    "\n",
    "    # phase 2\n",
    "    dcost_dah2 = np.dot(dcost_dzo, w[2].T)\n",
    "    dah2_dzh2 = relu_der(zh2)\n",
    "    dzh2_dw1 = ah1.T\n",
    "    dw1 = np.dot(dzh2_dw1, dcost_dah2 * dah2_dzh2) / m\n",
    "    db1 = np.sum(dcost_dah2 * dah2_dzh2, axis=0) / m\n",
    "\n",
    "    # phase 3\n",
    "    dcost_dah1 = np.dot(dcost_dah2 * dah2_dzh2, w[1].T)\n",
    "    dah1_dzh1 = relu_der(zh1)\n",
    "    dzh1_dw0 = X.T\n",
    "    dw0 = np.dot(dzh1_dw0, dcost_dah1 * dah1_dzh1) / m\n",
    "    db0 = np.sum(dcost_dah1 * dah1_dzh1, axis=0) / m\n",
    "\n",
    "    return dw0, db0, dw1, db1, dw2, db2\n",
    "\n",
    "\n",
    "def update_weights(w, b, dw0, db0, dw1, db1, dw2, db2, lr):\n",
    "    w[0] = w[0] - lr * dw0\n",
    "    b[0] = b[0] - lr * db0\n",
    "\n",
    "    w[1] = w[1] - lr * dw1\n",
    "    b[1] = b[1] - lr * db1\n",
    "\n",
    "    w[2] = w[2] - lr * dw2\n",
    "    b[2] = b[2] - lr * db2\n",
    "\n",
    "    return w, b\n",
    "\n",
    "\n",
    "\n",
    "### Train NN ###\n",
    "\n",
    "def my_multiout_neural_network(X_train, y_train, X_test, y_test, lr, epochs, batch_size):\n",
    "    start_time = time.time()\n",
    "    error_list = []\n",
    "    input_len = X_train.shape[1]\n",
    "    output_len = y_train.shape[1]\n",
    "    \n",
    "    # Structure 784, 200, 50, 10\n",
    "    w, b = define_parameters([input_len, 200, 50, output_len])\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # break epoch up into batches of 128\n",
    "        for X_batch, y_batch in batch_generator(X_train, y_train, batch_size):\n",
    "            \n",
    "            zh1, ah1, zh2, ah2, ao = predictions(w, b, X_batch)\n",
    "            # calculate loss\n",
    "            cost = cross_entropy_loss(ao, y_batch)\n",
    "            dw0, db0, dw1, db1, dw2, db2 = find_derivatives(w, b, X_batch, y_batch, zh1, ah1, zh2, ah2, ao)\n",
    "            # minimize loss based on gradients\n",
    "            w, b = update_weights(w, b, dw0, db0, dw1, db1, dw2, db2, lr)\n",
    "        \n",
    "        error_list.append(cost)\n",
    "        if epoch % 50 == 0:\n",
    "            print(f'Epoch {epoch}: cost = {cost}')\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    \n",
    "    train_accuracy = calculate_accuracy(X_train, y_train, w, b)\n",
    "    test_accuracy = calculate_accuracy(X_test, y_test, w, b)\n",
    "\n",
    "    return w, b, error_list, training_time, train_accuracy, test_accuracy\n",
    "\n",
    "# hyperparameters\n",
    "lr = 0.01\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "w, b, error_list, training_time, train_accuracy, test_accuracy = my_multiout_neural_network(X_train_flattened, y_train_one_hot, X_test_flattened, y_test_one_hot, lr, epochs, batch_size)\n",
    "\n",
    "print(f\"Training Time: {training_time:.4f} seconds\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97202b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
